{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "from streamlit_jupyter import StreamlitPatcher, tqdm\n",
    "\n",
    "sp = StreamlitPatcher()\n",
    "sp.jupyter()  # register streamlit with jupyter-compatible wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_JUPYTER_NOTEOOK = sp.registered_methods != set()\n",
    "if IN_JUPYTER_NOTEOOK:\n",
    "    print(\"We're running inside of a Jupyter Notebook\")\n",
    "else:\n",
    "    print(\"We're running outside of a Jupyter Notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Streamlit Application UI\n",
    "st.set_page_config(page_title=\"Nixpkgs RAG Chatbot\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"ðŸ¤– Nixpkgs RAG Chatbot\")\n",
    "st.caption(\"A conversational AI assistant for the Nixpkgs manual.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "\n",
    "def scrape_and_cache(url, cache_filepath):\n",
    "  if os.path.exists(cache_filepath):\n",
    "    with open(cache_filepath, 'r') as f:\n",
    "      cached_data = json.load(f)\n",
    "      docs = [Document(page_content=item['page_content'], metadata=item['metadata']) for item in cached_data]\n",
    "      print(\"cached Data is found\")\n",
    "      return docs\n",
    "  else:\n",
    "    print(f\"Caching not found, Scraping from: {url}\")\n",
    "    loader = WebBaseLoader(web_paths=[url]) # edit for the nix page\n",
    "    docs= loader.load()\n",
    "\n",
    "    os.makedirs(os.path.dirname(cache_filepath), exist_ok=True)\n",
    "    serializable_docs = []\n",
    "    for doc in docs:\n",
    "      serializable_docs.append({\n",
    "          'page_content': doc.page_content,\n",
    "          'metadata' : doc.metadata #check if metadatas\n",
    "      })\n",
    "\n",
    "    with open(cache_filepath, 'w') as f:\n",
    "      json.dump(serializable_docs, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"scraped data is cached to: {cache_filepath}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "web_path = \"https://nixos.org/manual/nixpkgs/stable/\"\n",
    "cache_dir = \"scraped_data_cache\"\n",
    "cache_file = os.path.join(cache_dir, \"nix_docs.json\")\n",
    "docs = scrape_and_cache(web_path, cache_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# vectorstore_nix = Chroma.from_documents(documents=splits, embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
    "vectorstore_nix = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore_nix.as_retriever()\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "## Change to HTMLHeaderTextSplitter\n",
    "# @st.cache_resource\n",
    "# def setup_vector_store():\n",
    "#     docs = scrape_and_cache(web_path, cache_file)\n",
    "#     html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "#     # Use the embedding model parameters.\n",
    "\n",
    "#     splits = text_splitter.split_documents(docs)\n",
    "#     embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#     vectorstore_nix = Chroma.from_documents(\n",
    "#         documents=splits,\n",
    "#         embedding=embedding,\n",
    "#     )\n",
    "#     return html_splitter, vectorstore_nix\n",
    "    # print(\"the length of the split is\", len(splits))\n",
    "#for i in splits:\n",
    "#  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add docs to vector DB using Chroma DB\n",
    "# # from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# headers_to_split_on = [\n",
    "#     (\"h1\", \"Header 1\"),\n",
    "#     (\"h2\", \"Header 2\"),\n",
    "#     (\"h3\", \"Header 3\"),\n",
    "# ]\n",
    "# # chunk_size = MAX_SEQ_LENGTH - HF_EOS_TOKEN_LENGTH\n",
    "# # chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "# html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "# child_splitter = RecursiveCharacterTextSplitter(\n",
    "#     # Set a really small chunk size, just to show.\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200,\n",
    "#     length_function=len,\n",
    "#     is_separator_regex=False,\n",
    "# )\n",
    "\n",
    "# docs = scrape_and_cache(web_path, cache_file)\n",
    "\n",
    "# def process_html_splits(html_splitter, headers_to_split_on):\n",
    "#     html_header_splits = []\n",
    "#     for doc in docs:\n",
    "#         splits = html_splitter.split_text(doc.page_content)\n",
    "#         for split in splits:\n",
    "#             # Add the source URL and header values to the metadata\n",
    "#             metadata = {}    \n",
    "#             new_text = split.page_content    \n",
    "#             for header_name, metadata_header_name in headers_to_split_on:    \n",
    "#                 header_value = new_text.split(\"Â¶ \")[0].strip()    \n",
    "#                 metadata[header_name] = header_value    \n",
    "#                 try:\n",
    "#                     new_text = new_text.split(\"Â¶ \")[1].strip()    \n",
    "#                 except:\n",
    "#                     break\n",
    "#             split.metadata = {\n",
    "#                 **metadata,\n",
    "#                 \"source\": doc.metadata[\"source\"]}\n",
    "#             split.page_content = split.page_content\n",
    "#         html_header_splits.extend(splits)\n",
    "#     return html_header_splits\n",
    "\n",
    "# processed_splits = process_html_splits(html_splitter, headers_to_split_on)\n",
    "# splits = child_splitter.split_documents(processed_splits)\n",
    "\n",
    "# vectorstore_nix = Chroma.from_documents(documents=splits, embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
    "# retriever = vectorstore_nix.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Auugmentation\n",
    "# # fetch the documents from the vector DB and then along with question whcih is a context send it to the\n",
    "\n",
    "# #https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=05726ff1-dd0c-4484-9c9c-cc8927681d12 # prompt from the lanchain hub\n",
    "\n",
    "# from langchain import hub\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "     \n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\") # default model is being used here\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "# llm = ChatOllama(\n",
    "#     model=\"deepseek-r1:1.5b\",\n",
    "#     reasoning=True,\n",
    "# )\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough # RunnablePassthrough is used when you want to pass the input as it is.\n",
    "from langchain_core.output_parsers import StrOutputParser # the output from llm has lot of info so to get only the correct content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"As a highly knowledgeable Nix Manual assistant, your role is to accurately interpret Nix related queries and \n",
    "provide responses using our specialized Nix Manual database. Follow these directives to ensure optimal user interactions:\\\n",
    "1. Precision in Answers: Respond solely with information directly relevant to the user's query from our Nix Manual database. \n",
    "    Refrain from making assumptions or adding extraneous details.\\\n",
    "2. Handling Off-topic Queries: For questions unrelated to Nix Manual (e.g., general knowledge questions like \"Why is the sky blue?\"), \n",
    "    politely inform the user that the query is outside the chatbotâ€™s scope and suggest redirecting to Nix Manual-related inquiries.\\\n",
    "3. Contextual Accuracy: Ensure responses are directly related to the Nix Manual query, utilizing only pertinent \n",
    "    information from our database.\\\n",
    "4. Relevance Check: If a query does not align with our Nix Manual database, guide the user to refine their \n",
    "    question or politely decline to provide an answer.\\\n",
    "5. Avoiding Duplication: Ensure no response is repeated within the same interaction, maintaining uniqueness and \n",
    "    relevance to each user query.\\\n",
    "6. Streamlined Communication: Eliminate any unnecessary comments or closing remarks from responses. Focus on\n",
    "    delivering clear, concise, and direct answers.\\\n",
    "7. Avoid Non-essential Sign-offs: Do not include any sign-offs like \"Best regards\" or \"NixBot\" in responses.\\\n",
    "8. One-time Use Phrases: Avoid using the same phrases multiple times within the same response. Each \n",
    "    sentence should be unique and contribute to the overall message without redundancy.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = ChatMessageHistory()\n",
    "    return st.session_state.messages\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(conversational_rag_chain.invoke(\n",
    "#     {\"input\": \"what are the best supported platforms?\"},\n",
    "#     config={\n",
    "#         \"configurable\": {\"session_id\": \"abc123\"}\n",
    "#     },\n",
    "# )[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = ChatMessageHistory()\n",
    "    st.session_state.messages.add_ai_message(\"Hello! I'm your assistant for the Nixpkgs manual. How can I help you today?\")\n",
    "\n",
    "for message in st.session_state.messages.messages:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(message.content)\n",
    "    elif isinstance(message, AIMessage):\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(message.content)\n",
    "\n",
    "# st.chat_input() is not implemented in streamlit-jupyter, so fall back to input()\n",
    "input_function = input if IN_JUPYTER_NOTEOOK else st.chat_input\n",
    "if prompt := input_function(\"Ask me a question...\"):\n",
    "    st.session_state.messages.add_user_message(prompt)\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            response = conversational_rag_chain.invoke(\n",
    "                {\"input\": prompt},\n",
    "                config={\"configurable\": {\"session_id\": \"streamlit_session_id\"}}\n",
    "            )\n",
    "            st.markdown(response[\"answer\"])\n",
    "            st.session_state.messages.add_ai_message(response[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
